---
layout:     post
title:      EVT ASPLOS 2024
subtitle:   EVT Accelerating Deep Learning Training with Epilogue Visitor Tree
date:       2025-04-05
author:     Treaseven
header-img: img/bg2.jpg
catalog: true
tags:
    - Graph-level Optimization
    - Operator-level Optimization
    - Partitioning Algorithms
---

### Challenges
在优化神经网络模型训练，进行编译优化所遇到的挑战
1. 现有算子编译器不能生成融合库能充分发挥性能同时适应各种各样的融合模式
2. 现有方法主要关注前向和后向优化，很少关注损失函数
3. 分割算法不能找到合适和最优的分割图

### Design

**Graph-level Optimizations**<br>
损失消除： 在反向传输计算不需要计算损失值；只要当用户需要分析训练过程的时候损失值才计算<br>
分解<br>
规约消除

**Partitioner**<br>
整数线性编程问题

<img width="500" height="450" src="../img/post-evt-ilp.png"/>

解决上面ILP问题会导致O(n^3)个决策变量，提出两步解决方法：
1. 将V个节点集分割不联合成分{C1,...,Cm}
2. 重建每个成本的边确保解决方案的有效性


<img width="500" height="450" src="../img/post-evt-algorithm1.png"/>

<img width="500" height="650" src="../img/post-evt-algorithm2.png"/>


<img width="500" height="450" src="../img/post-evt-adding-edges.png"/>

**Operator-level Optimization**

<img width="500" height="300" src="../img/post-evt-operator-compiler.png"/>


### Evaluation
benchmark: BERT-Large、VIT、ResNet-50、XML-CNN、GCN<br>
baseline: Torch Inductor、NVFuser


<img width="1000" height="300" src="../img/post-evt-training-speedup.png"/>


### Reference
[EVT: Accelerating Deep Learning Training with Epilogue Visitor Tree](https://dl.acm.org/doi/pdf/10.1145/3620666.3651369)
