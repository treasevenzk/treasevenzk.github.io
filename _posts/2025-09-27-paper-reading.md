---
layout:     post
title:      Paper reading
subtitle:   Compiler Optimization
date:       2025-09-27
author:     Treaseven
header-img: img/bg29.jpg
catalog: true
tags:
    - AI Compiler
---

### Paper [![number](https://img.shields.io/badge/91-148-blue.svg?style=flat)](https://github.com/home-assistant/home-assistant-iOS/blob/master/LICENSE)

---
Compiler

- <img src="https://img.shields.io/badge/12-pages-green.svg" alt="12-pages" align="left"> [Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines](https://dl.acm.org/doi/pdf/10.1145/2499370.2462176) - Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo Durand, Saman Amarasinghe, PLDI, 2013

- <img src="https://img.shields.io/badge/29-pages-green.svg" alt="29-pages" align="left"> [The tensor algebra compiler](https://dl.acm.org/doi/pdf/10.1145/3133901) - Fredrik Kjolstad, Shoaib Kamil, Stephen Chou, David Lugato, Saman Amarasinghe, OOPSLA, 2017

- <img src="https://img.shields.io/badge/37-pages-green.svg" alt="37-pages" align="left"> [Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions](https://arxiv.org/pdf/1802.04730) - Nicolas Vasilache, Oleksandr Zinenko, Theodoros Theodoridis, Priya Goyal, Zachary DeVito, William S. Moses, Sven Verdoolaege, Andrew Adams, Albert Cohen, arxiv, 2018

- <img src="https://img.shields.io/badge/3-pages-green.svg" alt="3-pages" align="left"> [Intel nGraph: An Intermediate Representation, Compiler, and Executor for Deep Learning](https://arxiv.org/pdf/1801.08058) - Scott Cyphers, Arjun K. Bansal, Anahita Bhiwandiwalla, Jayaram Bobba, Matthew Brookhart, Avijit Chakraborty, Will Constable, Christian Convey, Leona Cook, Omar Kanawi, Robert Kimball, Jason Knight, Nikolay Korovaiko, Varun Kumar, Yixing Lao, Christopher R. Lishka, Jaikrishnan Menon, Jennifer Myers, Sandeep Aswath Narayana, Adam Procter, Tristan J. Webb, arxiv, 2018

- <img src="https://img.shields.io/badge/12-pages-green.svg" alt="12-pages" align="left"> [Glow: Graph Lowering Compiler Techniques for Neural Networks](https://arxiv.org/pdf/1805.00907) - Nadav Rotem, Jordan Fix, Saleem Abdulrasool, Garret Catron, Summer Deng, Roman Dzhabarov, Nick Gibson, James Hegeman, Meghan Lele, Roman Levenstein, Jack Montgomery, Bert Maher, Satish Nadathur, Jakob Olesen, Jongsoo Park, Artem Rakhov, Misha Smelyanskiy, Man Wang, arxiv, 2018

- <img src="https://img.shields.io/badge/13-pages-green.svg" alt="13-pages" align="left"> [Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code](https://arxiv.org/pdf/1804.10694) - Riyadh Baghdadi, Jessica Ray, Malek Ben Romdhane, Emanuele Del Sozzo, Abdurrahman Akkas, Yunming Zhang, CGO, 2019

- <img src="https://img.shields.io/badge/16-pages-green.svg" alt="16-pages" align="left"> [TASO: Optimizing Deep Learning Computation with Automatic Generation of Graph Substitutions](https://arxiv.org/pdf/2101.08458) - Zhihao Jia, Oded Padon, James Thomas, Todd Warszawski, Matei Zaharia, Alex Aiken, SOSP, 2019

- <img src="https://img.shields.io/badge/10-pages-green.svg" alt="10-pages" align="left"> [Triton: an intermediate language and compiler for tiled neural network computations](https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf) - Philippe Tillet, H. T. Kung, David Cox, MAPL, 2019

- <img src="https://img.shields.io/badge/34-pages-green.svg" alt="34-pages" align="left"> [The Deep Learning Compiler: A Comprehensive Survey](https://arxiv.org/pdf/2002.03794) - Mingzhen Li, Yi Liu, Xiaoyan Liu, Qingxiao Sun, Xin You, Hailong Yang, TPDS, 2020

- <img src="https://img.shields.io/badge/20-pages-green.svg" alt="20-pages" align="left"> [A Tensor Compiler for Unified Machine Learning Prediction Serving](https://www.usenix.org/system/files/osdi20-nakandala.pdf) - Supun Nakandalac, Karla Saurm, Gyeong-In Yus, Konstantinos Karanasosm, Carlo Curinom, Markus Weimerm, Matteo Interlandi, OSDI, 2020

- <img src="https://img.shields.io/badge/41-pages-green.svg" alt="41-pages" align="left"> [Relay: A High-Level IR for Deep Learning](https://d1wqtxts1xzle7.cloudfront.net/93907908/1904.08368v1-libre.pdf?1667936889=&response-content-disposition=inline%3B+filename%3DRelay_A_High_Level_IR_for_Deep_Learning.pdf&Expires=1759370551&Signature=WKLOpsDWtCclKYwzKwAgQPMYreq03RLfqTg0r6N5vVOrAkBK7mz8y4HNtOcUufWnyNiXmC2OE7YUpPX84KbN9UFCSk1yIUSxhotwrxejGxsht1TYntu0wNRP4GQX8vX~VeFDdLac-oInQ~hywqHm1UNk7tELK57nN5JWIVGnnItiquI-nVfYy~FO4WCjBKbwlvBc8x7MypTjIqB~1gjrCyaNgPRxZJO8mx6VNzFk2c-j8dRj~d5Osuf-bd1-y01iR7D1GCyolEhAO2Lg31d4yYJki~RkJ1oJZ4V~bzNtWltXvAxWYlpAua8CV0sXpv8mq1NhcOq9Zu6GfY2VRnv-eA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA) - Jared Roesch, Steven Lyubomirsky, Marisa Kirisame, Josh Pollock, Logan Weber, Ziheng Jiang, Tianqi Chen, Thierry Moreau, Zachary Tatlock, arxiv, 2019

- <img src="https://img.shields.io/badge/13-pages-green.svg" alt="13-pages" align="left"> [MLIR: Scaling Compiler Infrastructure for Domain Specific Computation](https://rcs.uwaterloo.ca/~ali/cs842-s23/papers/mlir.pdf) - Chris Lattner, Mehdi Amini, Uday Bondhugula, Albert Cohen, Andy Davis, Jacques Pienaar, CGO, 2021

- <img src="https://img.shields.io/badge/13-pages-green.svg" alt="13-pages" align="left"> [UNIT: Unifying Tensorized Instruction Compilation](https://arxiv.org/pdf/2101.08458) - Jian Weng, Animesh Jain, Jie Wang, Leyuan Wang, Yida Wang, Tony Nowatzki, CGO, 2021

- <img src="https://img.shields.io/badge/16-pages-green.svg" alt="16-pages" align="left"> [AKG: Automatic Kernel Generation for Neural Processing Units using Polyhedral Transformations](https://01.me/files/AKG/akg-pldi21.pdf) - Jie Zhao, Bojie Li, Wang Nie, Zhen Geng, Renwei Zhang, Xiong Gao, Bin Cheng, Chen Wu, Yun Cheng, Zheng Li, Peng Di, Kun Zhang, Xuefeng Jin, PLDI, 2021

- <img src="https://img.shields.io/badge/18-pages-green.svg" alt="18-pages" align="left"> [Uncovering Nested Data Parallelism and Data Reuse in DNN Computation with FractalTensor](https://dl.acm.org/doi/pdf/10.1145/3694715.3695961) - Siran Liu, Chengxiang Qi, Ying Cao, Chao Yang, Weifang Hu, Xuanhua Shi, Fan Yang, Mao Yang, SOSP, 2024


---
Operator Optimizing 

- <img src="https://img.shields.io/badge/15-pages-green.svg" alt="15-pages" align="left"> [FlexTensor: An Automatic Schedule Exploration and Optimization Framework for Tensor Computation on Heterogeneous System](https://www.cl.cam.ac.uk/~ey204/teaching/ACS/R244_2022_2023/papers/ZHENG_ASPLOS_2020.pdf) - Size Zheng, Yun Liang, Shuo Wang, Renze Chen, Kaiwen Sheng, ASPLOS, 2020

- <img src="https://img.shields.io/badge/12-pages-green.svg" alt="12-pages" align="left"> [Fireiron: A Data-Movement-Aware Scheduling Language for GPUs](https://dl.acm.org/doi/pdf/10.1145/3410463.3414632) - Bastian Hagedorn, Archibald Samuel Elliott, Henrik Barthels, Rastislav Bodik, PACT, 2020

- <img src="https://img.shields.io/badge/15-pages-green.svg" alt="15-pages" align="left"> [Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9251965) - Jie Zhao, Peng Di, MICRO, 2020

- <img src="https://img.shields.io/badge/14-pages-green.svg" alt="14-pages" align="left"> [Tensor Program Optimization with Probabilistic Programs](https://proceedings.neurips.cc/paper_files/paper/2022/file/e894eafae43e68b4c8dfdacf742bcbf3-Paper-Conference.pdf) - Junru Shao, Xiyou Zhou, Siyuan Feng, Bohan Hou, Ruihang Lai, Hongyi Jin, Wuwei Lin, Masahiro Masuda, Cody Hao Yu, Tianqi Chen, NIPS, 2022

- <img src="https://img.shields.io/badge/15-pages-green.svg" alt="15-pages" align="left"> [Hidet: Task-Mapping Programming Paradigm for Deep Learning Tensor Programs
](https://dl.acm.org/doi/pdf/10.1145/3575693.3575702) - Yaoyao Ding, Cody Hao Yu, Bojian Zheng, Yizhi Liu, Yida Wang, Gennady Pekhimenko, ASPLOS, 2023

- <img src="https://img.shields.io/badge/15-pages-green.svg" alt="15-pages" align="left"> [Heron: Automatically Constrained High-Performance Library Generation for Deep Learning Accelerators](https://dl.acm.org/doi/pdf/10.1145/3582016.3582061) - Jun Bi, Qi Guo, Xiaqing Li, Yongwei Zhao, Yuanbo Wen, Yuxuan Guo, Enshuai Zhou, Xing Hu, Zidong Du, Ling Li, Huaping Chen, Tianshi Chen, ASPLOS, 2023

- <img src="https://img.shields.io/badge/15-pages-green.svg" alt="15-pages" align="left"> [Felix: Optimizing Tensor Programs with Gradient Descent](https://dl.acm.org/doi/pdf/10.1145/3620666.3651348) - Yifan Zhao, Hashim Sharif, Vikram Adve, Sasa Misailovic，ASPLOS, 2024

- <img src="https://img.shields.io/badge/12-pages-green.svg" alt="12-pages" align="left"> [Sifter: An Efficient Operator Auto-Tuner With Speculative Design Space Exploration for Deep Learning Compiler](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10643602) - Qianhe Zhao, Rui Wang, Yi Liu, Hailong Yang, Zhongzhi Luan, Depei Qian, TC, 2025

- <img src="https://img.shields.io/badge/25-pages-green.svg" alt="25-pages" align="left"> [Swit: High Parallelism Program Generation of Tensor Operators for Accelerating Deep Learning Inference](https://dl.acm.org/doi/pdf/10.1145/3762660) - Xiyue Yu, Jun Bi, Yuanbo Wen, Jianxing Xu, Di Huang, Jiaming Guo, Wei Li, Zidong Du, Jing Li, Tianshi Chen, Qi Guo, TACO, 2025

- <img src="https://img.shields.io/badge/13-pages-green.svg" alt="13-pages" align="left"> [Gensor: A Graph-based Construction Tensor Compilation Method for Deep Learning](https://arxiv.org/pdf/2502.11407) - Hangda Liu, Boyu Diao, Yu Yang, Wenxin Chen, Xiaohui Peng, Yongjun Xu, axriv, 2025


Graph Optimizing 

- <img src="https://img.shields.io/badge/6-pages-green.svg" alt="6-pages" align="left"> [GTuner: Tuning DNN Computations on GPU via Graph Attention Network](https://dl.acm.org/doi/pdf/10.1145/3489517.3530584) - Qi Sun, Xinyun Zhang, Hao Geng, Yuxuan Zhao, Yang Bai, Haisheng Zheng, Bei Yu, DAC, 2022

- <img src="https://img.shields.io/badge/11-pages-green.svg" alt="11-pages" align="left"> [Exploiting Subgraph Similarities for Efficient Auto-tuning of Tensor Programs](https://dl.acm.org/doi/pdf/10.1145/3605573.3605596) - Mingzhen Li, Hailong Yang, Shanjun Zhang, Fengwei Yu, Ruihao Gong, Yi Liu, Zhongzhi Luan, Depei Qian, ICPP, 2023

- <img src="https://img.shields.io/badge/14-pages-green.svg" alt="14-pages" align="left"> [GTA: Generating high-performance tensorized program with dual-task scheduling](https://www.sciencedirect.com/science/article/pii/S1383762125000311) - Anxing Xie, Yonghua Hu, Yaohua Wang, Zhe Li, Yuxiang Gao, Zenghua Cheng, JSA, 2025

- <img src="https://img.shields.io/badge/14-pages-green.svg" alt="14-pages" align="left"> [Pruner: A Draft-then-Verify Exploration Mechanism to Accelerate Tensor Program Tuning](https://www.sciencedirect.com/science/article/pii/S1383762125000311) - Liang Qiao, Jun Shi, Xiaoyu Hao, Xi Fang, Sen Zhang, Minfan Zhao, Ziqi Zhu, Junshi Chen, Hong An, Xulong Tang, Bing Li, Honghui Yuan, Xinyang Wang, ASPLOS, 2025

- <img src="https://img.shields.io/badge/18-pages-green.svg" alt="18-pages" align="left"> [Bayesian Code Diffusion for Efficient Automatic Deep Learning Program Optimization](https://www.usenix.org/system/files/osdi25-jeong.pdf) - Isu Jeong, Seulki Lee, OSDI, 2025


Instruction Optimizing 

- <img src="https://img.shields.io/badge/16-pages-green.svg" alt="16-pages" align="left"> [FreeTensor: A Free-Form DSL with Holistic Optimizations for Irregular Tensor Programs](https://dl.acm.org/doi/pdf/10.1145/3519939.3523448) - Shizhi Tang, Jidong Zhai, Haojie Wang, Lin Jiang, Liyan Zheng, Zhenhao Yuan, Chen Zhang, PLDI, 2022

- <img src="https://img.shields.io/badge/14-pages-green.svg" alt="14-pages" align="left"> [AMOS: Enabling Automatic Mapping for Tensor Computations On Spatial Accelerators with Hardware Abstraction](https://dl.acm.org/doi/pdf/10.1145/3470496.3527440) - Size Zheng, Renze Chen, Anjiang Wei, Yicheng Jin, Qin Han, Liqiang Lu, Bingyang Wu, Xiuhong Li, Shengen Yan, Yun Liang, ISCA, 2022

- <img src="https://img.shields.io/badge/12-pages-green.svg" alt="12-pages" align="left"> [Graphene: An IR for Optimized Tensor Computations on GPUs](https://dl.acm.org/doi/pdf/10.1145/3582016.3582018) - Bastian Hagedorn, Bin Fan, Hanfeng Chen, Cris Cecka, Michael Garland, Vinod Grover, ASPLOS, 2023

- <img src="https://img.shields.io/badge/14-pages-green.svg" alt="14-pages" align="left"> [TensorIR: An Abstraction for Automatic Tensorized Program Optimization](https://dl.acm.org/doi/pdf/10.1145/3575693.3576933) - Siyuan Feng, Bohan Hou, Hongyi Jin, Wuwei Lin, Junru Shao, Ruihang Lai, Zihao Ye, Lianmin Zheng, Cody Hao Yu, Yong Yu, Tianqi Chen, ASPLOS, 2023

- <img src="https://img.shields.io/badge/17-pages-green.svg" alt="17-pages" align="left"> [Mosaic: Exploiting Instruction-Level Parallelism on Deep Learning Accelerators with iTex Tessellation](https://dl.acm.org/doi/pdf/10.1145/3676641.3716262) - Jianxing Xu, Yuanbo Wen, Zikang Liu, Ruibai Xu, Tingfeng Ruan, Jun Bi, Rui Zhang, Di Huang, Xinkai Song, Yifan Hao, Xing Hu, Zidong Du, Chongqing Zhao, Jiang Jie, Qi Guo, ASPLOS, 2025

- <img src="https://img.shields.io/badge/17-pages-green.svg" alt="17-pages" align="left"> [IntelliGen: Instruction-Level Auto-tuning for Tensor Program with Monotonic Memory Optimization](https://dl.acm.org/doi/pdf/10.1145/3696443.3708967) - Zixuan Ma, Haojie Wang, Jingze Xing, Shuhong Huang, Liyan Zheng, Chen Zhang, Huanqi Cao, Kezhao Huang, Mingshu Zhai, Shizhi Tang, Penghan Wang, Jidong Zhai, CGO, 2025


CostModel 

- <img src="https://img.shields.io/badge/12-pages-green.svg" alt="12-pages" align="left"> [Value Learning for Throughput Optimizationof Deep Neural Networks](https://proceedings.mlsys.org/paper_files/paper/2021/file/a7e5da037a0afc90fa84386586929a26-Paper.pdf) - Benoit Steiner, Chris Cummins, Horace He, Hugh Leather, MLSys, 2021

- <img src="https://img.shields.io/badge/14-pages-green.svg" alt="14-pages" align="left"> [Tenset: A large-scale program performance dataset for learned tensor compilers](https://openreview.net/pdf?id=aIfp8kLuvc9) - Lianmin Zheng, Ruochen Liu, Junru Shao, Tianqi Chen, Joseph E. Gonzalez, Ion Stoica, Ameer Haj Ali, NIPS, 2021

- <img src="https://img.shields.io/badge/16-pages-green.svg" alt="16-pages" align="left"> [DeepCuts: A Deep Learning Optimization Framework for Versatile GPU Workloads](https://dl.acm.org/doi/pdf/10.1145/3453483.3454038) - Wookeun Jung, Thanh Tuan Dao, Jaejin Lee, PLDI, 2021

- <img src="https://img.shields.io/badge/16-pages-green.svg" alt="16-pages" align="left"> [A Flexible Approach to Autotuning Multi-pass Machine Learning Compilers](https://mangpo.net/papers/xla-autotuning-pact2021.pdf) - Phitchaya Mangpo Phothilimthana, Amit Sabne, Nikhil Sarda, Karthik Srinivasa Murthy, Yanqi Zhou, Christof Angermuellere, PACT, 2021

- <img src="https://img.shields.io/badge/16-pages-green.svg" alt="16-pages" align="left"> [Mind Mappings: Enabling Efficient Algorithm-Accelerator Mapping Space Search](https://dl.acm.org/doi/pdf/10.1145/3445814.3446762) - Kartik Hegde, Po-An Tsai, Sitao Huang, Vikas Chandra, Angshuman Parashar, Christopher W. Fletcher, ASPLOS, 2021

- <img src="https://img.shields.io/badge/13-pages-green.svg" alt="13-pages" align="left"> [CoSA: Scheduling by Constrained Optimization for Spatial Accelerators](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9499855) - Qijing Huang, Minwoo Kang, Grace Dinh, Thomas Norell, Aravind Kalaiah, James Demmel, John Wawrzynek, Yakun Sophia Shao, ISCA, 2021

- <img src="https://img.shields.io/badge/7-pages-green.svg" alt="7-pages" align="left"> [Moses: Efficient Exploitation of Cross-device Transferable Features for Tensor Program Optimization](https://dl.acm.org/doi/pdf/10.1145/3453483.3454038) - Zhihe Zhao, Xian Shuai, Yang Bai, Neiwen Ling, Nan Guan, Zhenyu Yan, Guoliang Xing, arxiv, 2022

- <img src="https://img.shields.io/badge/13-pages-green.svg" alt="13-pages" align="left"> [Effective Performance Modeling and Domain-Specific Compiler Optimization of CNNs for GPUs](https://dl.acm.org/doi/pdf/10.1145/3559009.3569674) - Yufan Xu, Qiwei Yuan, Erik Curtis Barton, Rui Li, P. Sadayappan, Aravind Sukumaran-Rajam, PACT, 2022

- <img src="https://img.shields.io/badge/15-pages-green.svg" alt="15-pages" align="left"> [One-Shot Tuner for Deep Learning Compilers](https://dl.acm.org/doi/pdf/10.1145/3497776.3517774) - Jaehun Ryu, Eunhyeok Park, Hyojin Sung, CC, 2022

- <img src="https://img.shields.io/badge/12-pages-green.svg" alt="12-pages" align="left"> [Transfer-Tuning: Reusing Auto-Schedules for Efficient Tensor Program Code Generation](https://dl.acm.org/doi/pdf/10.1145/3559009.3569682) - Perry Gibson, José Cano, PACT, 2022

- <img src="https://img.shields.io/badge/13-pages-green.svg" alt="13-pages" align="left"> [TLP: A Deep Learning-Based Cost Model for Tensor Program Tuning](https://www.sciencedirect.com/science/article/pii/S138376212400242X?via%3Dihub) - Yi Zhai, Yu Zhang, Shuo Liu, Xiaomeng Chu, Jie Peng, Jianmin Ji, Yanyong Zhang, ASPLOS, 2023

- <img src="https://img.shields.io/badge/13-pages-green.svg" alt="13-pages" align="left"> [DOPpler: Parallel Measurement Infrastructure for Auto-Tuning Deep Learning Tensor Programs](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10132048) - Damian Borowiec, Gingfung Yeung, Adrian Friday, Richard Harper, Peter Garraghan, TPDS, 2023

- <img src="https://img.shields.io/badge/13-pages-green.svg" alt="13-pages" align="left"> [Fasor: A Fast Tensor Program Optimization Framework for Efficient DNN Deployment](https://dl.acm.org/doi/pdf/10.1145/3650200.3656631) - Hanxian Huang, Xin Chen, Jishen Zhao, ICS, 2024

- <img src="https://img.shields.io/badge/18-pages-green.svg" alt="18-pages" align="left"> [Enabling Tensor Language Model to Assist in Generating High-Performance Tensor Programs for Deep Learning](https://www.usenix.org/system/files/osdi24-zhai.pdf) - Massinissa Merouani, Khaled Afif Boudaoud, Iheb Nassim Aouadj, Nassim Tchoulak, Islem Kara Bernou, Hamza Benyamina, Fatima Benbouzid-Si Tayeb, Karima Benatchba, Hugh Leather, OSDI, 2024

- <img src="https://img.shields.io/badge/13-pages-green.svg" alt="13-pages" align="left"> [LOOPer: A Learned Automatic Code Optimizer For Polyhedral
Compilers](https://arxiv.org/pdf/2403.11522) - Yi Zhai, Sijia Yang, Keyu Pan, Renwei Zhang, Shuo Liu, Chao Liu, Zichun Ye, Jianmin Ji, Jie Zhao, Yu Zhang, Yanyong Zhang, arxiv, 2024

- <img src="https://img.shields.io/badge/6-pages-green.svg" alt="6-pages" align="left"> [Crop: An Analytical Cost Model for Cross-Platform Performance Prediction of Tensor Programs](https://dl.acm.org/doi/pdf/10.1145/3649329.3658249) - Xinyu Sun, Yu Zhang, Shuo Liu, Yi Zhai, DAC, 2024

- <img src="https://img.shields.io/badge/13-pages-green.svg" alt="13-pages" align="left"> [Accelerated Auto-Tuning of GPU Kernels for Tensor Computations](https://dl.acm.org/doi/pdf/10.1145/3650200.3656626) - Chendi Li, Yufan Xu, Sina Mahdipour Saravani, Ponnuswamy Sadayappan, ICS, 2024

- <img src="https://img.shields.io/badge/14-pages-green.svg" alt="14-pages" align="left"> [GTA: Generating high-performance tensorized program with dual-task scheduling](https://pdf.sciencedirectassets.com/271017/1-s2.0-S1383762124X00144/1-s2.0-S1383762125000311/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEIn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQDzN7sU68Tw1y6IaWa7qX5la1p5vTwZTxxHe8BCeSxqVwIhAKox6R%2B9B2uewTlrUTfejJMAbxWkGfe05ijhXJSGZmdYKrIFCCEQBRoMMDU5MDAzNTQ2ODY1IgxxTz2ZzN1Hy4lLc2QqjwWo4h9iyw4X39BFxHdK2YQEYpd4QxrD7S1KJEljt%2BTUEmEIx8M8qz%2F83moGPjo5OPXrcTzX4d0isA63ZlalTZMFbWyYSB7uYow4hjyeP3HGv%2Bp9FSLSaq9znX07fbECun5aFsB0%2B4GK7o6m2uj533QvHAK4aNS7xs8RspUWAjqpuQoFO8s1FT0sWkqOfV5jr0pOozTDK2YOxtx4urJ1XU2%2FBpfB57sKQTTMhj4piarMYERoI%2BAfuyYNrTWsxTgSRFcAam0rTHi2od%2By%2BRY%2BBLWLDiMLo8PRf6gzNOl%2FDUoWT6QNPnxKtf9X8tXT%2FP9K%2B6%2Fj7AJFYHYdq5CZKt3XTE2DRDSXkcg%2FppPnOuHLXGSXqKnfpeqbTBiuF1lC6vEMxLzAM0K829B8c5a8PQ96E5AWGPDlcn97i81jxMF36Ix7qpDMEuUs18cgx2pQ%2FjeqcPp4uVNc3RPsHbc0GGjvitpx0ioeFYJhxao1HtM9lDeCJNJ6lLVRYi2emCLcoUVIvVsg5oQioQUCyFmv%2F42P9j5RUi%2FFBuH%2FSpgoiWPYWMAWg4ftnLRqxmNH1wgqSAdE7t%2BKs0bw64fGYr1y6BCKTqUZKbMnroIPebJQKFlkcZnTP7vWMKhYxdI16Pjsm%2BsFBaOSfuWNs8z1l4de0DgvQFfWQpmZ2TD0Qzp7p7%2BZdaPyln2PaIMK%2BzdDMq4waSH4aYdKIF71EWH8%2Ffx%2FIwLJr2KS9EXa7au9frrm3tTmCWiog%2BeI53W9BpxqtByOsT5Ke7IQNPrRQodB7jSTtowl9735Y6K2A3Sslfn9eMMu84ds4teyhTxyWBTdqLi67r%2BMxNaHCrg5ua4%2FI%2Fny6h3NXbVIGRF5OELvkw7k%2BCkCw%2FIhMOCH98YGOrABmIZ1e1OGfy6wBONcegMJt9OlOHKq3i2CPp00HCidqsuyXAyvyNsAAf1kqv6ClzwJTDlYHxIOlGpEGeyPA2getxpbB89nz4rtzFp2p3kaz9qkfDF5LkqMDOu9jjJpwI8jLuF1lH723NJkLqtH4i2iilkIqGIXDn0YWHQR79gO6icQJfGcdZDKeXIbrfdy94iFcyrZqDrIFdrSAX%2F0JJWE%2FDLRr7SfRpPfm9aCkuY4qc4%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20251002T014958Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYTCMSQYIV%2F20251002%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=e86d456ed06c1a57b729324f1f6aafc83b95905073efff75926c73ac7643101d&hash=b73b0e8d26e2650ab12d4fa74312baad6e26dcf923637801514eacea518a677a&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1383762125000311&tid=spdf-d0013a73-563a-446f-b52d-14fb5725dc68&sid=183750e0235d8642079888a30ec3e8a18709gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0f155f5d5f535701035757&rr=98808bd79bed2b8d&cc=us) - Anxing Xie, Yonghua Hu, Yaohua Wang, Zhe Li, Yuxiang Gao, Zenghua Cheng, JSA, 2025

- <img src="https://img.shields.io/badge/12-pages-green.svg" alt="12-pages" align="left"> [Multi-level Machine Learning-Guided Autotuning for Efficient Code Generation on a Deep Learning Accelerator](https://dl.acm.org/doi/pdf/10.1145/3735452.3735538) - JooHyoung Cha, Munyoung Lee, Jinse Kwon, Jemin Lee, Yongin Kwon, LCTES, 2025

- <img src="https://img.shields.io/badge/11-pages-green.svg" alt="11-pages" align="left"> [NLTSP: A cost model for tensor program tuning using nested loop trees](https://www.sciencedirect.com/science/article/pii/S138376212400242X?via%3Dihub) - Xinghe Qin, Yunchun Li, Fengxu Lin, Wei Li, JSA, 2025

- <img src="https://img.shields.io/badge/26-pages-green.svg" alt="26-pages" align="left"> [GenCNN: A Partition-Aware Multi-Objective Mapping Framework for CNN Accelerators Based on Genetic Algorithm](https://www.sciencedirect.com/science/article/pii/S138376212400242X?via%3Dihub) - Yudong Mu, Zhihua Fan, Wenming Li, Zhiyuan Zhang, Xuejun An, Dongrui Fan, Xiaochun Ye, TACO, 2025

- <img src="https://img.shields.io/badge/16-pages-green.svg" alt="16-pages" align="left"> [A Learned Performance Model With Transfer Learning Across GPUs on Tensorized Instructions](https://www.cse.cuhk.edu.hk/~byu/papers/J141-TPDS2025-ATFormer.pdf) - Yang Bai, Mingjun Li, Wendong Xu, Bei Yu, TPDS, 2025


Operator Fusion

- <img src="https://img.shields.io/badge/16-pages-green.svg" alt="16-pages" align="left"> [FlashAttention: Fast and
Memory-Efficient Exact Attention with IO-Awareness](https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf) - Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré, NIPS, 2022


- <img src="https://img.shields.io/badge/14-pages-green.svg" alt="14-pages" align="left"> [FlashAttention-2:
Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/pdf/2307.08691) - Tri Dao, arxiv, 2023


- <img src="https://img.shields.io/badge/28-pages-green.svg" alt="28-pages" align="left"> [FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](https://proceedings.neurips.cc/paper_files/paper/2024/file/7ede97c3e082c6df10a8d6103a2eebd2-Paper-Conference.pdf) - Jay Shah, Ganesh Bikshandi, Ying Zhang , Vijay Thakkar, Pradeep Ramani, Tri Dao, NIPS, 2024

- <img src="https://img.shields.io/badge/18-pages-green.svg" alt="18-pages" align="left"> [Rammer: Enabling holistic deep learning compiler optimizations with {rTasks}](https://www.usenix.org/system/files/osdi20-ma.pdf) - Lingxiao Ma, Zhiqiang Xie, Zhi Yang, Jilong Xue, Youshan Miao, Wei Cui, Wenxiang Hu, Fan Yang, Lintao Zhang, Lidong Zhou, OSDI, 2020

- <img src="https://img.shields.io/badge/14-pages-green.svg" alt="14-pages" align="left"> [FusionStitching: Boosting Memory Intensive Computations for Deep Learning Workloads](https://arxiv.org/pdf/2009.10924) -  Zhen Zheng, Pengzhan Zhao, Guoping Long, Feiwen Zhu, Kai Zhu, Wenyi Zhao, Lansong Diao, Jun Yang, Wei Lin, arxiv, 2021

- <img src="https://img.shields.io/badge/16-pages-green.svg" alt="16-pages" align="left"> [DNNFusion: accelerating deep neural networks execution with advanced operator fusion](https://dl.acm.org/doi/pdf/10.1145/3453483.3454083) - Wei Niu, Jiexiong Guan, Yanzhi Wang, Gagan Agrawal, Bin Ren, PLDI, 2021

- <img src="https://img.shields.io/badge/19-pages-green.svg" alt="19-pages" align="left"> [PET: Optimizing Tensor Programs with Partially Equivalent Transformations and Automated Corrections](https://www.usenix.org/system/files/osdi21-wang-haojie.pdf) - Haojie Wang, Jidong Zhai, Mingyu Gao, Zixuan Ma, Shizhi Tang, Liyan Zheng, Yuanzhi Li, Kaiyuan Rong, Yuanyong Chen, Zhihao Jia, OSDI, 2021

- <img src="https://img.shields.io/badge/15-pages-green.svg" alt="15-pages" align="left"> [AStitch: Enabling a New Multi-dimensional Optimization Space for Memory-Intensive ML Training and Inference on Modern SIMT Architectures](https://jamesthez.github.io/files/astitch-asplos22.pdf) - Zhen Zheng, Xuanda Yang, Pengzhan Zhao, Guoping Long, Kai Zhu, Feiwen Zhu, Wenyi Zhao, Xiaoyong Liu, Jun Yang, Jidong Zhai, Shuaiwen Leon Song, Wei Lin, ASPLOS, 2022

- <img src="https://img.shields.io/badge/17-pages-green.svg" alt="17-pages" align="left"> [ROLLER: Fast and Efficient Tensor Compilation for Deep Learning](https://www.usenix.org/system/files/osdi22-zhu.pdf) - Hongyu Zhu, Ruofan Wu, Yijia Diao, Shanbin Ke, Haoyu Li, Chen Zhang, Jilong Xue, Lingxiao Ma, Yuqing Xia, Wei Cui, Fan Yang, Mao Yang, OSDI, 2022

- <img src="https://img.shields.io/badge/19-pages-green.svg" alt="19-pages" align="left"> [Apollo: Automatic Partition-based Operator Fusion through Layer by Layer Optimization](https://dl.acm.org/doi/pdf/10.1145/3519939.3523448) - Jie Zhao, Xiong Gao, Ruijie Xia, Zhaochuang Zhang, Deshi Chen, Lei Chen, Renwei Zhang, Zhen Geng, Bin Cheng, Xuefeng Jin, MLSys, 2022

- <img src="https://img.shields.io/badge/13-pages-green.svg" alt="13-pages" align="left"> [Collage: Seamless Integration of Deep Learning Backends with Automatic Placement](https://arxiv.org/pdf/2111.00655) - Byungsoo Jeon, Sunghyun Park, Peiyuan Liao, Sheng Xu, Tianqi Chen, Zhihao Jia, PACT, 2022

- <img src="https://img.shields.io/badge/13-pages-green.svg" alt="13-pages" align="left"> [Bolt: Bridging the Gap between Auto-tuners and Hardware-native Performance](https://proceedings.mlsys.org/paper_files/paper/2022/file/1f8053a67ec8e0b57455713cefdd8218-Paper.pdf) - Jiarong Xing, Leyuan Wang, Shang Zhang, Jack Chen, Ang Chen, Yibo Zhu, MLSys, 2022

- <img src="https://img.shields.io/badge/16-pages-green.svg" alt="16-pages" align="left"> [Optimizing Deep Learning Inference via Global Analysis and Tensor Expressions](https://dl.acm.org/doi/pdf/10.1145/3617232.3624858) - Chunwei Xia, Jiacheng Zhao, Qianqi Sun, Zheng Wang, Yuan Wen, Teng Yu, Xiaobing Feng, Huimin Cui, ASPLOS, 2023

- <img src="https://img.shields.io/badge/14-pages-green.svg" alt="14-pages" align="left"> [Chimera: An Analytical Optimizing Framework for Effective Compute-intensive Operators Fusion](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10071018) - Size Zheng, Siyuan Chen, Peidi Song, Renze Chen, Xiuhong Li, Shengen Yan, Dahua Lin, Jingwen Leng, Yun Liang, HPCA, 2023

- <img src="https://img.shields.io/badge/18-pages-green.svg" alt="18-pages" align="left"> [EINNET: Optimizing Tensor Programs with Derivation-Based Transformations](https://www.usenix.org/system/files/osdi23-zheng.pdf) - Liyan Zheng, Haojie Wang, Jidong Zhai, Muyan Hu, Zixuan Ma, Tuowei Wang, Shuhong Huang, Xupeng Miao, Shizhi Tang, Kezhao Huang, Zhihao Jia, OSDI, 2023

- <img src="https://img.shields.io/badge/19-pages-green.svg" alt="19-pages" align="left"> [Welder: Scheduling Deep Learning Memory Access via Tile-graph](https://www.usenix.org/system/files/osdi23-shi.pdf) - Yining Shi, Zhi Yang, Jilong Xue, Lingxiao Ma, Yuqing Xia, Ziming Miao, Yuxiao Guo, Fan Yang, Lidong Zhou, OSDI, 2023

- <img src="https://img.shields.io/badge/15-pages-green.svg" alt="15-pages" align="left"> [MCFuser: High-Performance and Rapid Fusion of Memory-Bound Compute-Intensive Operators](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10793220) - Zheng Zhang, Donglin Yang, Xiaobo Zhou, Dazhao Cheng, SC, 2024

- <img src="https://img.shields.io/badge/15-pages-green.svg" alt="15-pages" align="left"> [Optimal Kernel Orchestration for Tensor Programs with Korch](https://dl.acm.org/doi/pdf/10.1145/3620666.3651383) - Zheng Zhang, Donglin Yang, Xiaobo Zhou, Dazhao Cheng, ASPLOS, 2024


- <img src="https://img.shields.io/badge/14-pages-green.svg" alt="14-pages" align="left"> [FlashTensor: Optimizing Tensor Programs by Leveraging Fine-grained Tensor Property](https://dl.acm.org/doi/pdf/10.1145/3710848.3710864) - Runxin Zhong, Yuyang Jin, Chen Zhang, Kinman Lei, Shuangyu Li, Jidong Zhai, PPoPP, 2025

- <img src="https://img.shields.io/badge/16-pages-green.svg" alt="16-pages" align="left"> [SpaceFusion: Advanced Deep Learning Operator Fusion via Space-Mapping Graph](https://dl.acm.org/doi/pdf/10.1145/3689031.3696087) - Liang Zhu, Jianguo Yao, Haibing Guan, EuroSys, 2025

- <img src="https://img.shields.io/badge/27-pages-green.svg" alt="27-pages" align="left"> [OptiFX: Automatic Optimization for Convolutional Neural Networks with Aggressive Operator Fusion on GPUs](https://dl.acm.org/doi/pdf/10.1145/3716876) - Xueying Wang, Shigang Li, Hao Qian, Fan Luo, Zhaoyang Hao, Tong Wu, Ruiyuan Xu, Huimin Cui, Xiaobing Feng, Guangli Li, Jingling Xue, TACO, 2025

- <img src="https://img.shields.io/badge/15-pages-green.svg" alt="15-pages" align="left"> [Optimizing Deep Learning Inference Efficiency through Block Dependency Analysis](https://dl.acm.org/doi/pdf/10.1145/3676641.3716264) - Zhanyuan Di, Leping Wang, En Shao, Zhaojia Ma, Ziyi Ren, Feng Hua, Lixian Ma, Jie Zhao, Guangming Tan, Ninghui Sun, ASPLOS, 2025

- <img src="https://img.shields.io/badge/19-pages-green.svg" alt="19-pages" align="left"> [Mirage: A Multi-Level Superoptimizer for Tensor Programs](https://www.usenix.org/system/files/osdi25-wu-mengdi.pdf) - Mengdi Wu, Xinhao Cheng, Shengyu Liu, Chunan Shi, Jianan Ji, Man Kit Ao, Praveen Velliengiri, Xupeng Miao, Oded Padon, Zhihao Jia, OSDI, 2025

Other reference 33 + 81 =114

Devlin J, Chang MW, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understanding. InProceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers) 2019 Jun (pp. 4171-4186).

Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I. Language models are unsupervised multitask learners. OpenAI blog. 2019 Feb 24;1(8):9.

Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, Neelakantan A, Shyam P, Sastry G, Askell A, Agarwal S. Language models are few-shot learners. Advances in neural information processing systems. 2020;33:1877-901.

Achiam J, Adler S, Agarwal S, Ahmad L, Akkaya I, Aleman FL, Almeida D, Altenschmidt J, Altman S, Anadkat S, Avila R. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. 2023 Mar 15.

Meta AI. The llama 4 herd: The beginning of a new era of natively multimodal ai innovation. https://ai. meta. com/blog/llama-4-multimodal-intelligence/, checked on. 2025 Apr;4(7):2025.

Grok X. Beta—The Age of Reasoning Agents| xAI, 2025. URL https://x. ai/news/grok-3. 3.

Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, Neelakantan A, Shyam P, Sastry G, Askell A, Agarwal S. Language models are few-shot learners. Advances in neural information processing systems. 2020;33:1877-901.

Child R, Gray S, Radford A, Sutskever I. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509. 2019 Apr 23.

Beltagy I, Peters ME, Cohan A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150. 2020 Apr 10.

Shazeer N, Mirhoseini A, Maziarz K, Davis A, Le Q, Hinton G, Dean J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538. 2017 Jan 23.

Choquette J, Gandhi W. Nvidia a100 gpu: Performance & innovation for gpu computing. In2020 IEEE Hot Chips 32 Symposium (HCS) 2020 Aug 1 (pp. 1-43). IEEE Computer Society.

Luo W, Fan R, Li Z, Du D, Wang Q, Chu X. Benchmarking and dissecting the nvidia hopper gpu architecture. In 2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS) 2024 May 27 (pp. 656-667).

Smith R. NVIDIA Blackwell Architecture and B200/B100 Accelerators Announced: Going Bigger With Smaller Data. Retrieved September. 2024;1:2024.

Hao C, Zhang X, Li Y, Huang S, Xiong J, Rupnow K, Hwu WM, Chen D. FPGA/DNN co-design: An efficient design methodology for IoT intelligence on the edge. InProceedings of the 56th Annual Design Automation Conference 2019 2019 Jun 2 (pp. 1-6).

Chen YH, Yang TJ, Emer J, Sze V. Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices. IEEE Journal on Emerging and Selected Topics in Circuits and Systems. 2019 Apr 11;9(2):292-308.

Jouppi NP, Young C, Patil N, Patterson D, Agrawal G, Bajwa R, Bates S, Bhatia S, Boden N, Borchers A, Boyle R. In-datacenter performance analysis of a tensor processing unit. InProceedings of the 44th annual international symposium on computer architecture 2017 Jun 24 (pp. 1-12).

Jouppi NP, Yoon DH, Ashcraft M, Gottscho M, Jablin TB, Kurian G, Laudon J, Li S, Ma P, Ma X, Norrie T. Ten lessons from three generations shaped google’s tpuv4i: Industrial product. In2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA) 2021 Jun 14 (pp. 1-14).

Liao H, Tu J, Xia J, Liu H, Zhou X, Yuan H, Hu Y. Ascend: a scalable and unified architecture for ubiquitous deep neural network computing: Industry track paper. In2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA) 2021 Feb 27 (pp. 789-801). 

Gao W, Zhan J, Fox G, Lu X, Stanzione D, editors. Benchmarking, Measuring, and Optimizing: Second BenchCouncil International Symposium, Bench 2019, Denver, CO, USA, November 14–16, 2019, Revised Selected Papers. Springer Nature; 2020 Jun 9.

Li J, Jiang Z. Performance analysis of cambricon mlu100. InInternational Symposium on Benchmarking, Measuring and Optimization 2019 Nov 14 (pp. 57-66). Cham: Springer International Publishing.

Jia Z, Tillman B, Maggioni M, Scarpazza DP. Dissecting the graphcore ipu architecture via microbenchmarking. arXiv preprint arXiv:1912.03413. 2019 Dec 7.

Paszke A. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703. 2019.

Abadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, Devin M, Ghemawat S, Irving G, Isard M, Kudlur M. {TensorFlow}: a system for {Large-Scale} machine learning. In12th USENIX symposium on operating systems design and implementation (OSDI 16) 2016 (pp. 265-283).

Bi R, Xu T, Xu M, Chen E. Paddlepaddle: A production-oriented deep learning platform facilitating the competency of enterprises. In2022 IEEE 24th Int Conf on High Performance Computing & Communications; 8th Int Conf on Data Science & Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys) 2022 Dec 18 (pp. 92-99). 

Chen L. Deep learning and practice with mindspore. Springer Nature; 2021 Aug 17.

Leary C, Wang T. XLA: TensorFlow, compiled. TensorFlow Dev Summit. 2017 Feb;2(3).

NVIDIA. 2019. TensorRT Github repository. https://github.com/NVIDIA/TensorRT. Accessed February 4, 2020

Intel oneAPI Deep Neural Network Library. https://github.com/oneapi-src/oneDNN.

Nvidia CuBlas. https://developer.nvidia.com/cublas.

Chen T, Guestrin C. Xgboost: A scalable tree boosting system. InProceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining 2016 Aug 13 (pp. 785-794).

Introduction to Intel deep learning boost on second generation Intel Xeon scalable processors. https://software.intel.com/content/www/us/en/develop/articles/introductionto-intel-deep-learning-boost-on-second-generation-intel-xeonscalable.html, 2019.

Exploring the Arm dot product instructions. https://community.arm.com/developer/tools-software/tools/b/toolssoftware-ides-blog/posts/exploring-the-arm-dot-product-instructions, 2017.

Nvidia tensor cores. https://www.nvidia.com/en-us/data-center/tensorcores/, 2020.

张洪滨, 周旭林, 邢明杰, 武延军, 赵琛. AutoConfig: 面向深度学习编译优化的自动配置机制. 软件学报. 2024 Jan 5;35(6):2668-86.

李颖颖, 赵捷, 庞建民. 多面体模型中分裂分块算法的设计与实现. 计算机学报. 2020;43(6):1010-23.

曾军, 寇明阳, 郑惜元, 姚海龙, 孙富春.(2023).TVM_T:基于TVM的高性能神经网络训练编译器. 中国科学:信息科学(12),2458-2471.

韩林;王一帆;李嘉楠;高伟.一种基于TVM的自动调度搜索优化方法[J].计算机科学,2025(3):268-276.