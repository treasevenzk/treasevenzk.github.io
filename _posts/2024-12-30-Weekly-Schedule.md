---
layout:     post
title:      Weekly Schedule
subtitle:   plan for every week
date:       2024-12-30
author:     Treaseven
header-img: img/bg32.png
catalog: true
tags:
    - Weekly Schedule
---


### 12.30-1.5进度
*** 论文阅读计划 ***
- * ~~Interstellar: Using Halide’s Scheduling Language to Analyze DNN Accelerators~~
- * ~~Analytical Characterization and Design Space Exploration for Optimization of CNNs~~
- * ~~Mind mappings: enabling efficient algorithm-accelerator mapping space search~~
- * ~~Optimizing Deep Learning Inference via Global Analysis and Tensor Expressions~~
- * ~~AStitch: Enabling a New Multi-dimensional Optimization Space for Memory-intensive ML Training and Inference on Modern SIMT Architectures~~
- * ~~Hidet: Task-Mapping Programming Paradigm for Deep Learning Tensor Programs~~


*** 论文复现工作 ***

- ***CMLCompiler***<br>
决策树算法sklearn转换成tvm的DL model的流程如下：<br>
cmlcompiler.model &rightarrow; build_model &rightarrow; init_model &rightarrow; __init__ &rightarrow; __parse_params__ &rightarrow; convert_decision_tree &rightarrow; __get_algo &rightarrow; decision_tree_classifier &rightarrow;

- ***Mind Mappings***<br>

本周进度还是相对较慢，因为中间元旦放假两天，耽误了一些进度，下周还是需要更加加油！！！

### 1.6-1.12进度
*** 论文阅读计划 ***
- ROLLER: Fast and Efficient Tensor Compilation for Deep Learning
- SmartMem: Layout Transformation Elimination and Adaptation for Efficient DNN Execution on Mobile
- MonoNN: Enabling a New Monolithic Optimization Space for Neural Network Inference Tasks on Modern GPU-Centric Architectures
- Accelerated Auto-Tuning of GPU Kernels for Tensor Computations
- A full-stack search technique for domain optimized deep learning accelerators
- RAMMER: Enabling Holistic Deep Learning Compiler Optimizations with rTasks
- Bridging the Gap Between Domain-specific Frameworks and Multiple Hardware Devices 



### 1.13-1.19进度



### 1.20-1.22进度

