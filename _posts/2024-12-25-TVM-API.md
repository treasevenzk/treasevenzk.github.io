---
layout:     post
title:      TVM API
subtitle:   TVM API Explaination
date:       2024-12-25
author:     Treaseven
header-img: img/bg18.jpg
catalog: true
tags:
    - TVM API
---


### TVM学习资源
https://www.zhihu.com/people/yang-cheng-68-6/posts?page=2 <br>
http://giantpandacv.com/project/%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%96/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/TVM%20%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97/ <br>
https://discuss.tvm.apache.org/t/tvm/15159/16 <br>
https://www.cnblogs.com/wanger-sjtu <br>
https://tvm.hyper.ai/docs/ <br>
https://tvm.d2l.ai/ <br>


### tvm
* tvm.error
* tvm.ir
* tvm.instrument
* tvm.transform
* tvm.target <br>
```class tvm.target.Target```: 用于描述和管理目标设备的信息 ***tvm.target.Target(target, host=None)*** <br>
* tvm.driver

### tvm.runtime
* tvm.runtime
* tvm.runtime.ndarray
* tvm.runtime.relax_vm
* tvm.runtime.disco
* tvm.runtime.profiling

### tvm.relax
* tvm.relax
* tvm.relax.analysis
* tvm.relax.block_builder
* tvm.relax.frontend
* tvm.relax.op
* tvm.relax.transform

### tvm.tir
* tvm.tir
```tvm.tir.IntImm```: 表示整数常量的类 ***class tvm.tir.IntImm(dtype:str, value:int, span:Span|None = None)***
* tvm.tir.analysis
* tvm.tir.schedule
* tvm.tir.stmt_functor
* tvm.tir.transform

### tvm.te
* tvm.te <br>
```tvm.te.create_schedule```: 创建计算调度，接收一个或多个操作作为输入，返回一个schedule对象 ***tvm.te.create_schedule(ops)*** <br>
```tvm.te.placeholder```: 创建一个占位符张量 ***tvm.te.placeholder(shape, dtype=None, name='placeholder')*** <br>
```tvm.te.compute```: 定义张量计算操作的核心函数 ***tvm.te.compute(shape, fcompute, name='compute', tag='', attrs=None, varargs_names=None)*** <br>
```tvm.te.reduce_axis```: 创建归约操作的迭代轴 ***tvm.te.reduce_axis(dom, name='rv', thread_tag='', span=None)*** <br>
```tvm.te.sum```: 执行归约求和操作的函数 ***tvm.te.sum(expr, axis, where=None, init=None, *args)***
* tvm.te.hybrid
* tvm.topi

### tvm.meta_schedule
* tvm.meta_schedule

### tvm.dlight
* tvm.dlight

### Misc
* tvm.rpc
* tvm.contrib

### Legacy
* tvm.relay <br>
```tvm.relay.var```: 定义输入变量, ***tvm.relay.var(name_hint, type_annotation=None, shape=None, dtype='float32', span=None)*** <br>
```tvm.relay.greater```: 判断左侧值是否严格大于右侧值，返回一个布尔变量，***tvm.relay.greater(lhs, rhs)*** <br>
```tvm.relay.greater_equal```: 判断左侧值是否大于或等于右侧值，返回一个布尔变量，***tvm.relay.greater_equal(lhs, rhs)*** <br>
```class tvm.relay.Function```: 创建函数表达式的类，定义一个可计算的函数 ***class tvm.relay.Function(params, body, ret_type=None, type_params=None, attrs=None, span=None)*** 其方法 ***astext***,用于获取函数的文本格式表示 <br>
```tvm.relay.build```: 将Relay IR模块编译成可执行的形式 ***tvm.relay.build(ir_mod, target=None, target_host=None, executor=graph{"link-params":T.bool(False)}, runtime=cpp, workspace_memory_pools=None, constant_memory_pools=None, params=None, mod_name='default')*** <br>
```tvm.relay.abs```: 对输入的张量数据进行逐元素的绝对值运算 ***tvm.relay.abs(data)*** <br>
```tvm.relay.sum```: 计算数组元素求和的函数 ***tvm.relay.sum(data, axis=None, keepdims=False, exclude=False)*** <br>
```tvm.relay.divide```: 对两个张量执行逐元素除法 ***tvm.relay.divide(lhs, rhs)*** <br>
```tvm.relay.sqrt```: 计算张量元素平方根的函数 ***tvm.relay.sqrt(data)*** <br>
```tvm.relay.power```: 计算幂运算的函数 ***tvm.relay.power(lhs, rhs)*** <br>
```tvm.relay.unique```: 查找一维张量中唯一元素的函数 ***tvm.relay.unique(data, is_sorted=True, return_counts=False)*** <br>
```tvm.relay.multiply```: 执行乘法运算的函数,支持广播机制 ***tvm.relay.multiply(lhs, rhs)*** <br>
```tvm.relay.substract``: 执行减法运算的函数，支持广播机制 ***tvm.relay.subtract(lhs, rhs)*** <br>
* tvm.relay.frontend
* tvm.relay.nn <br>
```tvm.relay.nn.sparse_dense```: 执行稀疏矩阵与密集矩阵相乘的操作，返回类型为tvm.relay.Expr的计算结果矩阵 ***tvm.relay.nn.sparse_dense(dense_mat, sparse_mat, sparse_lhs=False)*** <br>
```tvm.relay.nn.dense```: 矩阵乘法运算 ***tvm.relay.nn.dense(data, weight, units=None, out_dtype='')*** <br>
```tvm.relay.nn.bias_add```: 将一维偏置张量加到输入数据的指定轴上 ***tvm.relay.nn.bias_add(data, bias, axis=1)*** <br>
* tvm.relay.vision
* tvm.relay.image
* tvm.relay.transform
* tvm.relay.analysis <br>
```tvm.relay.analysis.free_type_vars```: 获取表达式或类型中的自由类型变量(free type variables)、以后序深度优化搜索顺序返回 <br>
```tvm.relay.analysis.free_vars```: 获取表达式中的自由变量(free variables)、以后序深度优先搜索顺序返回 <br>
* tvm.relay.backend
* tvm.relay.dataflow_pattern
* tvm.relay.testing
* tvm.autotvm <br>
```class tvm.autotvm.measure.measure_methods.LocalBuilder```: 在本地机器上执行TVM编译任务，用于自动调优过程中，负责将优化后的计算图编译成可执行代码 ***tvm.autotvm.measure.measure_methods.LocalBuilder(timeout=10, n_parallel=None, build_kwargs=None, build_func='default', do_fork=False, runtime=None)*** <br>
```class tvm.autotvm.measure.measure_methods.RPCRunner```: 用于远程执行和测量性能的关键组件 ***tvm.autotvm.measure.measure_methods.RPCRunner(key, host, port, priority=1, timeout=10, n_parallel=None, number=4, repeat=3, min_repeat_ms=0, coldown_interval=0.1, enable_cpu_cache_flush=False, module_loader=None)*** <br>
```class tvm.autotvm.measure.measure_methods.LocalRunner```: 用于在本地设备上执行和测量代码性能的组件 ***tvm.autotvm.measure.measure_methods.LocalRunner(timeout=10, number=4, repeat=3, min_repeat_ms=0, coldown_interval=0.1, enable_cpu_cache_flush=False, module_loader=None)*** <br>
```class tvm.autotvm.measure.measure_option```: 用于配置性能测量选项的函数，将构建(builder)和运行(runner)的配置组合在一起，形成完整的测量方案 ***tvm.autotvm.measure.measure_option(builder, runner)*** <br>
```tvm.autotvm.measure.create_measure_batch```: 创建性能测量函数的工具，为自动调优系统提供性能评估能力 ***tvm.autotvm.measure.create_measure_batch(task, option)***
* tvm.auto_scheduler <br>
```tvm.auto_scheduler.extract_tasks```: 从给定的计算图中识别和提取可以优化的计算任务 ***tvm.auto_scheduler.extrac_tasks(mod, params, target, target_host=None, hardware_params=None, include_simple_tasks=False, dump_workload_to_dag_log=None, opt_level=3, other_targets=None)*** <br>
```class tvm.auto_scheduler.TaskScheduler```: 用于在同时调优多个任务时分配时间资源 ***tvm.auto_scheduler.TaskScheduler(tasks, task_weights=None, objective_func=None, strategy='gradient', load_model_file: str|None = None, load_log_file: str|None = None, alpha:float = 0.2, beta:float = 2, gamma:float = 0.5, backward_window_size: int = 3, callbacks=None)*** 其方法 ***tune(tune_option, search_policy='default', search_policy_params=None, adaptive_training=False, per_task_early_stopping=None)*** <br>
```class tvm.auto_scheduler.TuningOptions```: 用于控制性能调优选项的配置 ***tvm.auto_scheduler.TuningOptions(num_measure_trials=0, early_stopping=None, num_measures_per_round=64, verbose=1, builder='local', runner='local', measure_callbacks=None)*** <br>
```class tvm.auto_scheduler.LocalRunner```: 用于在本地CPU/GPU上测量程序运行时间 ***classtvm.auto_scheduler.LocalRunner(timeout=10, number=3, repeat=1, min_repeat_ms=100, cooldown_interval=0.0, enable_cpu_cache_flush=False, device=0)*** <br>
```class tvm.auto_scheduler.RecordToFile```: 用于将测量记录写入文件 ***tvm.auto_scheduler.RecordToFile(filename)*** <br>
```class tvm.auto_scheduler.ApplyHistoryBest``` 用于应用历史最佳配置的类  ***tvm.auto_scheduler.ApplyHistoryBest(records, n_lines=None, include_compatible=False)*** 其方法 获取特定目标和工作负载的记录条目 ***static get_workload_entry(best_records, target_key, workload_key)*** 加载调优记录 ***load(records, n_lines=None)*** 更新工作负载的配置 ***update(target, workload_key, state)*** <br>
* tvm.contrib.graph_executor <br>
```class tvm.contrib.graph_executor.GraphModule```: 运行编译后的模型 ***tvm.contrib.graph_executor.GraphModule(module)***,其方法 ***set_input(key=None, value=None, \*\*params)*** 设置输入数据  ***set_input_zero_copy(key=None, value=None, \*\*params)***  零拷贝方式设置输入 ***set_output_zero_copy(key, value)*** 设置零拷贝输出 ***run(\*\*input_dict)*** 获取输出 ***get_num_outputs()*** 获取输出数量 ***get_num_inputs()*** 获取输入数量  ***get_input(index, out=None)*** 获取指定输入 ***get_input_index(name)*** 获取输入索引 ***get_input_info()*** 获取输入信息  ***get_output(index, out=None)*** 获取输出 ***debug_get_output(node, out)*** 获取中间节点输出  ***load_params(params_bytes)*** 加载参数  ***share_params(other, params_bytes)*** 共享参数 ***benchmark(device, func_name='run', repeat=5, min_repeat_ms=None, limit_zero_time_iterations=100, end_to_end=False, cooldown_interval_ms=0, repeats_cooldown=1, \*\*kwargs)*** 性能基准测试

### Other API
* tvm.IRModule <br>
```tvm.IRModule.from_expr```: 将Relay表达式转换为IR Module(中间表示模块)的函数 ***tvm.IRModule.from_expr(expr)***
* tvm.transform.PassContext <br>
```tvm.transform.PassContext(opt_level=3)```: 创建一个优化上下文，优化级别范围是0-3
* tvm.build <br>
```tvm.build```: 将计算描述编译成可执行的代码 ***tvm.build(schedule, args, target="llvm", target_host=None, name="default_functino", binds=None)*** 返回值是tvm.runtime.Module对象，其属性和功能: 时间测量 func.time_evaluator(func.entry_name, tvm.cpu(), number=10) 获取入口函数名 func.entry_name 获取已导出的函数列表 func.get_function_list() 获取特定函数 func.get_function("function_name") 保存到文件 func.save("compiler_func.o") 加载模块 func.export_library("compiled_lib.so") 从文件加载 tvm.runtime.load_module("compiled_lib.so") 获取源代码 func.get_source() 获取IR代码 func.get_ir() 设置线程数 func.set_num_threads 配置GPU工作组大小 func.set_cuda_props(max_shared_memory_per_block=49152) 模块类型 func.type_key 代码格式 func.format 是否为二进制格式 func.is_binary


### other important things
```
leaf_iter_vars: 叶子迭代变量
for v in stage.leaf_iter_vars:
    print("变量名", v.var.name)
    print("迭代范围", v.dom.min, v.dom.extent)
    print("迭代类型", v.iter_type)
    print("循环类型", v.for_type)
    print("线程标签", v.thread_tag)
```
```
stage: 表示计算阶段的重要对象
属性相关内容
- op相关
* stage.op: 当前阶段的操作
    * op.name: 操作的名称
    * op.axis: 空间维度的迭代轴
    * op.reduce_axis: 归约维度的迭代轴
    * op.input_tensors: 输入张量
    * op.output: 输出张量
- 迭代变量
* stage.leaf_iter_vars: 叶子迭代变量列表
* stage.all_iter_vars: 所有迭代变量列表
* stage.leaf_var_map: 叶子变量映射
- 调度信息
* stage.attach_type: 绑定类型
* stage.attach_ivar: 绑定的迭代变量
* stage.attach_stage: 绑定的计算阶段
* stage.store_pred: 存储谓词
* stage.compute_pred: 计算谓词
- 优化相关
* stage.schedule: 所属的调度对象
* stage.group: 计算组信息
* stage.scope: 存储域范围
* stage.is_output: 是否是输出阶段
* stage.is_specialized: 是否是特化的阶段
-----------------------------------------
常用方法
outer, inner = stage.split(axis, factor) # 分割迭代轴
stage.reorder(axis1, axis2, ...) # 重排序迭代轴
stage.compute_at(other_stage, axis) # 计算绑定
stage.store_at(other_stage, axis) # 存储绑定
stage.parallel(axis) # 并行化
stage.vectorize(axis) # 向量化
stage.unroll(axis) # 展开
stage.cache_read(tensor, scope, readers) # 缓存数据
stage.cache_write(tensor, scope) # 缓存数据
stage.fuse(axis1, axis2) # 融合操作
stage.bind(axis, thread_type) # 线程绑定
```