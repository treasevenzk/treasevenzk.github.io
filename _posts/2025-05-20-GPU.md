---
layout:     post
title:      GPU
subtitle:   GPU
date:       2025-05-20
author:     Treaseven
header-img: img/bg2.jpg
catalog: true
tags:
    - GPU
---

# GPU编程
**设备侧和主机侧**

GPU编程的思维是将GPU当做CPU的协同外设使用，通过GPU自身无法独立运行，需要CPU指定任务，分配数据，驱动运行，CPU称为主机侧，而GPU称为设备侧

**线程组织**

<img width="500" height="300" src="../img/post-gpu-thread.png"/>

线程(thread): 最基本的执行单元，线程包含独立寄存器状态和独立程序计数器 <br>
线程块(thread block): 由多个线程组成的集合，支持一维、二维或三维结构。线程块内的线程可以通过共享内存进行通信，线程块之间无法通过共享内存通信，但可通过全局内存进行数据交互 <br>
Warp: 硬件底层概念，CPU实际运行时将32个线程组成一个warp，同一warp内的线程同步执行相同的指令 <br>
线程块与warp的关系： warp是底层概念，NVIDIA的warp固定包含32个线程，warp是线程硬件调度的最小粒度，线程块是软件概念，线程块有多少个线程组成由代码指定。在运行时，硬件会将线程块中的线程32个为一组打包成多个warp进行调度，线程块里的线程数最好为32的整数倍，以避免为拼凑完整warp而自动分配无效线程造成资源浪费 <br>
网格(grid): 网格是所有线程块的集合，支持一维、二维或三维结构，覆盖整个计算任务的运行范围 <br>

|NVIDIA(CUDA) | AMD(OpenGL)|
|:---: | :---:|
|Grid | NDRange|
|Thread Block | Work Group|
|Warp | Wavefront|
|Thread | Work Item|

区别于NIVIDIA， AMD的一个wavefront由64个work item组成，线程块有时也被称为CTA

**线程块的计算**
一个线程块由多少个线程组成可以指定，与此不同的是，线程块本身的数量则是由计算规模决定的
B = (N + T - 1) / T  [B: 线程块数， N: 问题规模, T: 线程块内线程数]

**指定线程执行内核函数指令**
```
__global__ void saxpy(int n, float a, float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        y[i] = a * x[i] + y[i];
    }
}
```
每个线程分配一个其所属的向量元素，然后驱动线程分别完成计算
i为线程的编号，blockIdx是block在grid的坐标，blockDim则是block本身的尺寸threadIdx为thread在block上的坐标


**多维线程组织结构**

```
// 主机端调用代码
void launch_kernel_3d() {
    // 三维数据尺寸
    int dimX = 64    int dimY = 32    int dimZ = 16;
    // 定义三维线程块（Block）和网格（Grid）
    dim3 blockSize(8, 4, 4);  // 指定每个块包含8x4x4=128个线程
    dim3 gridSize(
        (dimX + blockSize.x - 1) / blockSize.x, // X方向块数
        (dimY + blockSize.y - 1) / blockSize.y, // Y方向块数
        (dimZ + blockSize.z - 1) / blockSize.z  // Z方向块数
    );
    // 启动内核函数
    kernel_3d<<<gridSize, blockSize>>>(d_data, dimX, dimY, dimZ);
}
```

```
// 核函数定义（处理三维数据）
__global__ void kernel_3d(float* data, int dimX, int dimY, int dimZ) {
    // 计算三维索引
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;
    int z = blockIdx.z * blockDim.z + threadIdx.z;

    if (x < dimX && y < dimY && z < dimZ) {
        // 处理三维数据（例如：三维矩阵元素操作）
        int idx = x + y * dimX + z * dimX * dimY; // 线程编号
        data[idx] *= 2.0f; // 示例：每个元素翻倍
    }
}
```

**SIMT**
SIMT: 通过统一指令只会多个线程并行处理不同数据
SIMD: 在同一时刻向多个数据元素执行同样的一条指令，SIMD范式常见的一种实现是CPU的向量化运算，将N份数据存储在向量寄存器里，执行一条指令，同时作用于向量寄存器里的每个数据 指令+操作数 -> 结果


<img width="500" height="200" src="../img/post-gpu-simt.png"/>


**指令集与编译**
SASS是GPU的机器指令集
PTX是一种中间表示形式位于高级GPU编程语言和低级机器指令集之间，PTX与GPU架构基本无耦合关系，本质上是从SASS上抽象出来的一种更上层的软件编程模型


# SIMT核心架构

<img width="1000" height="260" src="../img/post-gpu-simt-arch.png"/>

SIMT前端: 负责指令的获取、译码和发射、分支预测、以及线程的管理和调度
SIMD后端: 负责完成计算

**最小可用系统**

<img width="800" height="260" src="../img/post-gpu-simt-first.png"/>

最简GPU可以做到最简的指令执行功能: 即顺序执行每一条指令，一条指令执行完再执行下一条 <br>
Fetch: 取指令 <br>
Decode: 指令解码 <br>
SIMT Stack: SIMT堆栈，管理线程束的分支执行状态 <br>
Issue: 指令发射 <br>
ALU: 算数逻辑单元，代表执行计算的组件 <br>
MEM: 存储器访问单元，代表对L1 Cache、共享内存等各层级内存访问的管理 <br>



**动态指令调度以提高并发**

<img width="800" height="260" src="../img/post-gpu-simt-second.png"/>



**提高并发指令的数据供给效率**

<img width="1000" height="260" src="../img/post-gpu-simt-third.png"/>


# GPU内存模型
**CUDA内存模型**

寄存器、共享内存、本地内存、常量内存、纹理内存、全局内存

<img width="500" height="500" src="../img/post-gpu-memory-model.png"/>

每个线程都有自己的私有的本地内存；线程块有自己的共享内存，对线程块内的所有线程可见；所有线程都能访问读取常量内存和纹理内存，但不能写 <br>

寄存器: 在GPU内核函数内不加修饰声明一个变量，此变量就存储在寄存器中；在CPU内只有当前在计算的变量存储在寄存器中，其余在主存中，使用时传输至寄存器 <br>
本地内存: 核函数中符合存储在寄存器中但不能进入被核函数分配的寄存器空间中的变量存储在本地内存中 <br>
全局内存: 全局内存访问是对齐，也就是一次要读取指定大小(32, 64, 128)整数倍字节的内存，所以当线程束执行内存加载/存储时，需要满足的传输数量通常取决于两个因素(1. 跨线程的内存地址分布 2. 内存事务的对齐方式)


**内存管理**
内存分配和释放: cudaMalloc、cudaMemset、cudaFree
内存传输: cudaMemcpy
统一虚拟寻址、统一内存寻址


**内存访问模式**

核函数运行时需要从全局内存中读取数据，只有两种粒度: 128字节、32字节 <br>
对齐内存访问、合并内存访问 <br>
一次内存请求--也就是从内核函数发起请求，到硬件响应返回数据这个过程称为一个内存事务 <br>
一个内存事务的首个访问地址是缓存粒度的偶数倍的时候称之为对齐内存访问，非对齐访问就是除上述的其他情况，非对齐的内存访问会造成带宽浪费 <br>
当一个线程束内的线程访问的内存都在一个内存块里的时候，就会出现合并访问


<img width="500" height="200" src="../img/post-gpu-align.png"/>


<img width="500" height="200" src="../img/post-gpu-disalign.png"/>


**共享内存**
每个线程对共享内存的访问请求:
1. 最好的情况是当前线程束中的每个线程都访问一个不冲突的共享内存，这种情况，大家互不干扰，一个事务完成整个线程束的访问，效率最高
2. 当有访问冲突的时候，这时候一个线程束32个线程，需要32个事务
3. 如果线程束内32个线程访问同一个地址，那么一个线程访问完后以广播的形式告诉大家

共享内存有个特殊的形式是，分为32个同样大小的内存模型，称为存储体，可以同时访问，32个存储体的目的是对应一个线程束中有32个线程，这些线程在访问共享内存的时候，如果都访问不同存储体(无冲突)，那么一个事务就能够完成，否则需要多个内存事务，这样带宽利用率降低<br>
线程束访问共享内存
1. 并行访问，多地址访问多存储体
2. 串行访问，多地址访问同一存储体
3. 广播访问，单一地址读取单一存储体

共享内存的数据布局


Wave: GPU在同一时间能够并发执行的所有线程块的集合，由于GPU的SM数量有限，如果启动的Block数量超过GPU能同时处理的数量，就需要分成多个Wave来执行

|GPU型号|SM数量|每SM最大线程数|每SM最大Block数|每Wave最大Block数|
|:---:|:---:|:---:|:---:|:---:|
|V100|80|2048|32|160(Block大小为1024时)|
|A100|108|2048|32|216(Block大小为1024时)|
|RTX 4090|128|2048|32|256(Block大小为1024时)|